"""Document tools â€” template generation and validation.

Tools for creating structured documents (PRDs, stakeholder updates) and
validating content quality (citations, formatting).
"""

from __future__ import annotations

import logging
import re
from pathlib import Path
from datetime import datetime

from claude_agent_sdk import tool

logger = logging.getLogger(__name__)


@tool(
    "generate_prd_from_template",
    "Create a Product Requirements Document from structured template with Problem, Solution, Success Metrics, Dependencies, Timeline sections",
    {
        "feature_name": str,  # Name of the feature
        "problem": str,  # Problem statement
        "solution": str,  # Proposed solution
        "success_metrics": str,  # How to measure success (comma-separated or JSON array)
        "dependencies": str,  # Dependencies and blockers (optional)
        "timeline": str,  # Target timeline (optional)
    },
)
async def generate_prd_from_template(args: dict) -> dict:
    """Generate a structured PRD from template."""
    feature_name = args.get("feature_name", "Unnamed Feature")
    problem = args.get("problem", "")
    solution = args.get("solution", "")
    success_metrics = args.get("success_metrics", "")
    dependencies = args.get("dependencies", "None identified")
    timeline = args.get("timeline", "TBD")

    if not problem or not solution:
        return {"content": [{"type": "text", "text": "Problem and solution are required for PRD"}]}

    # Generate PRD using standard template
    prd = f"""# PRD: {feature_name}

**Status:** Draft
**Owner:** Product Team
**Created:** {datetime.now().strftime("%Y-%m-%d")}
**Last Updated:** {datetime.now().strftime("%Y-%m-%d")}

---

## Problem Statement

{problem}

## Proposed Solution

{solution}

## Success Metrics

"""

    # Parse success metrics
    if success_metrics:
        metrics_list = [m.strip() for m in success_metrics.split(",")]
        for metric in metrics_list:
            prd += f"- {metric}\n"
    else:
        prd += "- *(To be defined)*\n"

    prd += f"""
## Dependencies & Blockers

{dependencies}

## Timeline

{timeline}

## Open Questions

- *(Add any open questions or assumptions to validate)*

## Appendix

### Related Resources
- Linear ticket: *(Link to tracking issue)*
- Design mocks: *(Link to Figma/designs)*
- Technical spec: *(Link to tech design doc)*

---

*Generated by Velocity AI PM Agent*
"""

    return {"content": [{"type": "text", "text": prd}]}


@tool(
    "generate_stakeholder_update",
    "Generate a weekly or monthly stakeholder update with Shipped, In Progress, Upcoming, Blockers, and Metrics sections",
    {
        "period": str,  # e.g., "Week of Jan 15" or "Q1 2024"
        "shipped": str,  # Comma-separated or JSON array of completed items
        "in_progress": str,  # Current work
        "upcoming": str,  # Planned work
        "blockers": str,  # Current blockers (optional)
        "metrics": str,  # Key metrics update (optional)
    },
)
async def generate_stakeholder_update(args: dict) -> dict:
    """Generate structured stakeholder update."""
    period = args.get("period", datetime.now().strftime("Week of %B %d, %Y"))
    shipped = args.get("shipped", "")
    in_progress = args.get("in_progress", "")
    upcoming = args.get("upcoming", "")
    blockers = args.get("blockers", "None")
    metrics = args.get("metrics", "")

    update = f"""# Product Update: {period}

---

## ðŸš¢ Shipped

"""

    if shipped:
        shipped_items = [item.strip() for item in shipped.split(",")]
        for item in shipped_items:
            update += f"- {item}\n"
    else:
        update += "- *(Nothing shipped this period)*\n"

    update += "\n## ðŸ”„ In Progress\n\n"

    if in_progress:
        progress_items = [item.strip() for item in in_progress.split(",")]
        for item in progress_items:
            update += f"- {item}\n"
    else:
        update += "- *(No active work)*\n"

    update += "\n## ðŸ“… Upcoming\n\n"

    if upcoming:
        upcoming_items = [item.strip() for item in upcoming.split(",")]
        for item in upcoming_items:
            update += f"- {item}\n"
    else:
        update += "- *(No planned work)*\n"

    update += f"\n## ðŸš§ Blockers\n\n{blockers}\n"

    if metrics:
        update += f"\n## ðŸ“Š Key Metrics\n\n{metrics}\n"

    update += "\n---\n\n*Questions? Reach out to the product team.*\n"

    return {"content": [{"type": "text", "text": update}]}


@tool(
    "validate_document_citations",
    "Check that all claims in a document have source citations and calculate citation coverage percentage",
    {
        "document": str,  # The document text to validate
    },
)
async def validate_document_citations(args: dict) -> dict:
    """Validate that document claims are properly cited."""
    document = args.get("document", "")

    if not document:
        return {"content": [{"type": "text", "text": "Document text is required"}]}

    # Find all markdown links (citations)
    citation_pattern = r'\[([^\]]+)\]\(([^\)]+)\)'
    citations = re.findall(citation_pattern, document)

    # Find potential unsourced claims (sentences with strong claims but no nearby citation)
    # This is a heuristic - looks for definitive statements
    claim_indicators = [
        r'\d+%',  # Percentages
        r'\d+\s+users',  # User counts
        r'will\s+\w+',  # Future predictions
        r'shows?\s+that',  # Data claims
        r'according\s+to',  # References
        r'data\s+shows',  # Data claims
        r'research\s+indicates',  # Research claims
    ]

    lines = document.split('\n')
    uncited_claims = []

    for line_num, line in enumerate(lines, 1):
        # Skip headings, code blocks, and already-cited lines
        if line.startswith('#') or '```' in line or '[' in line and '](' in line:
            continue

        # Check if line contains claim indicators
        for pattern in claim_indicators:
            if re.search(pattern, line, re.IGNORECASE):
                # Check if there's a citation in this line or next 2 lines
                context_lines = lines[max(0, line_num-1):min(len(lines), line_num+2)]
                has_citation = any('[' in l and '](' in l for l in context_lines)

                if not has_citation:
                    uncited_claims.append({
                        "line": line_num,
                        "text": line.strip()[:100],  # First 100 chars
                    })
                break

    # Calculate coverage
    total_claims = len(uncited_claims) + len(citations)
    if total_claims == 0:
        coverage = 100.0  # No claims = perfectly cited
    else:
        coverage = (len(citations) / total_claims) * 100

    # Format output
    output = f"# Citation Validation Report\n\n"
    output += f"**Total Citations Found:** {len(citations)}\n"
    output += f"**Potential Uncited Claims:** {len(uncited_claims)}\n"
    output += f"**Citation Coverage:** {coverage:.1f}%\n\n"

    if coverage >= 90:
        output += "âœ… **Excellent** - Most claims are properly cited.\n"
    elif coverage >= 70:
        output += "âš ï¸ **Good** - Some claims could use citations.\n"
    else:
        output += "âŒ **Needs Improvement** - Many claims lack sources.\n"

    if citations:
        output += "\n## Citations Found\n\n"
        for text, url in citations[:10]:  # Show first 10
            output += f"- [{text}]({url})\n"
        if len(citations) > 10:
            output += f"\n*(and {len(citations) - 10} more)*\n"

    if uncited_claims:
        output += "\n## âš ï¸ Potential Uncited Claims\n\n"
        output += "These lines contain claims that may need citations:\n\n"
        for claim in uncited_claims[:10]:  # Show first 10
            output += f"**Line {claim['line']}:** {claim['text']}\n\n"
        if len(uncited_claims) > 10:
            output += f"*(and {len(uncited_claims) - 10} more)*\n"

    output += "\n**Recommendation:** "
    if uncited_claims:
        output += "Add source links for claims using markdown format: [source text](URL)\n"
    else:
        output += "All claims appear to be properly cited! âœ…\n"

    return {"content": [{"type": "text", "text": output}]}


@tool(
    "format_for_notion",
    "Convert markdown document to Notion-compatible format for future integration",
    {
        "markdown": str,  # Markdown document to convert
    },
)
async def format_for_notion(args: dict) -> dict:
    """Convert markdown to Notion-friendly format.

    Notion supports most markdown but has some quirks:
    - Uses ### for H1 (not #)
    - Supports callouts with >
    - Supports toggle lists
    - Handles tables differently
    """
    markdown = args.get("markdown", "")

    if not markdown:
        return {"content": [{"type": "text", "text": "Markdown text is required"}]}

    # Convert markdown to Notion format
    notion_text = markdown

    # Convert H1 to H3 (Notion's H1 is ###)
    notion_text = re.sub(r'^# ', '### ', notion_text, flags=re.MULTILINE)
    notion_text = re.sub(r'^## ', '#### ', notion_text, flags=re.MULTILINE)
    notion_text = re.sub(r'^### ', '##### ', notion_text, flags=re.MULTILINE)

    # Convert checkboxes
    notion_text = re.sub(r'- \[ \] ', '- [ ] ', notion_text)
    notion_text = re.sub(r'- \[x\] ', '- [x] ', notion_text, flags=re.IGNORECASE)

    # Add callout formatting hints
    notion_text = re.sub(
        r'^> (.+)$',
        r'> ðŸ’¡ \1',
        notion_text,
        flags=re.MULTILINE
    )

    output = f"""# Notion-Compatible Format

The document has been converted to Notion-compatible markdown.

**Changes made:**
- Adjusted heading levels (H1 â†’ H3, H2 â†’ H4, H3 â†’ H5)
- Added emoji indicators to callouts (>)
- Preserved links, code blocks, and lists
- Tables remain as markdown tables (manually convert in Notion)

**To import:**
1. Copy the formatted text below
2. In Notion, paste into a page
3. Notion will auto-convert markdown formatting
4. Manually adjust tables if needed

---

{notion_text}

---

**Note:** For best results, use Notion's markdown import feature or paste directly into a Notion page.
"""

    return {"content": [{"type": "text", "text": output}]}
